import requests
from bs4 import BeautifulSoup
import os
import sys
import io
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import re


def configure_console():
    """é…ç½®æ§åˆ¶å°ç¼–ç """
    if sys.platform == 'win32':
        import win32console
        win32console.SetConsoleOutputCP(65001)  # è®¾ç½®ä¸ºUTF-8
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')


def sanitize_filename(filename):
    """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
    return re.sub(r'[\\/*?:"<>|]', "_", filename)


def ensure_dirs():
    """ç¡®ä¿é…ç½®å’Œè¾“å‡ºç›®å½•å­˜åœ¨"""
    config_dir = './config'
    output_dir = './output'

    if not os.path.exists(config_dir):
        os.makedirs(config_dir)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    return config_dir, output_dir


def get_sitemap_paths():
    """è·å–å¤šä¸ªsitemapè·¯å¾„ï¼ˆæœ¬åœ°æˆ–è¿œç¨‹ï¼‰"""
    config_dir, _ = ensure_dirs()
    config_file = os.path.join(config_dir, 'sitemap_config.txt')

    # å°è¯•è¯»å–å†å²é…ç½®
    if os.path.exists(config_file):
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                paths = [line.strip() for line in f if line.strip()]
                if paths and all(
                        p.startswith(('http://', 'https://')) or os.path.exists(p)
                        for p in paths
                ):
                    print("\n=== æ£€æµ‹åˆ°å†å²é…ç½® ===")
                    for i, path in enumerate(paths, 1):
                        print(f"{i}. {path}")
                    use_history = input("\næ˜¯å¦ä½¿ç”¨è¿™äº›è·¯å¾„ï¼Ÿ(y/n, é»˜è®¤y): ").strip().lower()
                    if use_history in ('y', 'yes', ''):
                        return paths
        except Exception as e:
            print(f"âš ï¸ é…ç½®è¯»å–å¤±è´¥: {e}")

    # æ‰‹åŠ¨è¾“å…¥å¤šä¸ªè·¯å¾„
    print("\n=== è¯·è¾“å…¥å¤šä¸ªsitemapè·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œç©ºè¡Œç»“æŸï¼‰===")
    print("å¯æ¥å—ï¼š")
    print("1. æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ C:/sitemap.xml æˆ– ./sitemap.xmlï¼‰")
    print("2. ç½‘ç»œURLï¼ˆå¦‚ https://example.com/sitemap.xmlï¼‰")
    print("3. è¾“å…¥ç©ºè¡Œç»“æŸ")

    paths = []
    while True:
        path = input(f"è·¯å¾„ {len(paths) + 1}: ").strip()
        if not path:
            if paths:
                break
            print("âŒ è‡³å°‘è¾“å…¥ä¸€ä¸ªè·¯å¾„")
            continue

        # éªŒè¯è·¯å¾„
        if path.startswith(('http://', 'https://')):
            try:
                requests.head(path, timeout=3)
                paths.append(path)
            except:
                print("âŒ æ— æ³•è®¿é—®è¯¥URLï¼Œè¯·æ£€æŸ¥ç½‘ç»œå’Œåœ°å€")
        elif os.path.exists(path):
            if path.endswith('.xml'):
                paths.append(path)
            else:
                print("âŒ è¯·æä¾›XMLæ ¼å¼çš„æ–‡ä»¶")
        else:
            print("âŒ è·¯å¾„ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è¾“å…¥")

    # ä¿å­˜é…ç½®
    try:
        with open(config_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(paths))
    except Exception as e:
        print(f"âš ï¸ é…ç½®ä¿å­˜å¤±è´¥ï¼ˆä¸å½±å“æœ¬æ¬¡ä½¿ç”¨ï¼‰: {e}")

    return paths


def parse_sitemap(path, is_root=True):
    """è§£æå•ä¸ªsitemapæ–‡ä»¶/URLï¼Œæ”¯æŒåµŒå¥—sitemapç´¢å¼•"""
    try:
        if is_root:
            print(f"\næ­£åœ¨è§£æ: {path}")

        if path.startswith(('http://', 'https://')):
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(path, headers=headers, timeout=10)
            response.raise_for_status()
            content = response.content
        else:
            with open(path, 'rb') as f:
                content = f.read()

        soup = BeautifulSoup(content, 'lxml-xml')

        # æ£€æŸ¥æ˜¯å¦æ˜¯sitemapç´¢å¼•æ–‡ä»¶
        sitemapindex = soup.find_all('sitemap')
        if sitemapindex:
            if is_root:
                print("æ£€æµ‹åˆ°sitemapç´¢å¼•æ–‡ä»¶ï¼Œå¼€å§‹è§£æåµŒå¥—sitemap...")
            urls = []
            for sitemap in sitemapindex:
                loc = sitemap.find('loc')
                if loc and loc.text.strip():
                    nested_url = loc.text.strip()
                    print(f"æ­£åœ¨å¤„ç†åµŒå¥—sitemap: {nested_url}")
                    try:
                        nested_urls = parse_sitemap(nested_url, is_root=False)
                        urls.extend(nested_urls)
                    except Exception as e:
                        print(f"âŒ åµŒå¥—sitemapè§£æå¤±è´¥: {nested_url} - {str(e)}")
            return path, urls

        # æ™®é€šsitemapæ–‡ä»¶
        urls = [loc.text.strip() for loc in soup.find_all('loc') if loc.text.strip()]

        if not urls:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆURL")

        return path, urls

    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {path} - {e}")
        return path, []


def parse_multiple_sitemaps(paths):
    """è§£æå¤šä¸ªsitemapæ–‡ä»¶/URLï¼Œè¿”å›{è·¯å¾„: URLåˆ—è¡¨}çš„å­—å…¸"""
    sitemap_urls = {}
    with tqdm(total=len(paths), desc="è§£æsitemap", unit="file") as pbar:
        for path in paths:
            original_path, urls = parse_sitemap(path)
            if urls:
                sitemap_urls[original_path] = urls
                pbar.set_postfix_str(f"URLs: {sum(len(v) for v in sitemap_urls.values())}")
            pbar.update(1)
    return sitemap_urls


def fetch_title(url, verbose=False):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.title.string.strip() if soup.title else "æ— æ ‡é¢˜"
        if verbose:
            print(f"âœ… æˆåŠŸè·å–: {url} - {title}")
        return url, title
    except Exception as e:
        if verbose:
            print(f"âŒ è·å–å¤±è´¥: {url} - é”™è¯¯: {str(e)}")
        return url, f"è·å–æ ‡é¢˜å¤±è´¥: {str(e)}"


def fetch_titles_for_sitemap(urls, max_workers=10, progress_type='bar'):
    """ä¸ºå•ä¸ªsitemapçš„URLåˆ—è¡¨è·å–æ ‡é¢˜"""
    url_titles = []
    if progress_type == 'bar':
        with tqdm(total=len(urls), desc="è·å–æ ‡é¢˜", unit="URL") as pbar:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {executor.submit(fetch_title, url, False): url for url in urls}
                for future in as_completed(futures):
                    url = futures[future]
                    try:
                        url, title = future.result()
                        url_titles.append((url, title))
                        pbar.update(1)
                        pbar.set_postfix_str(f"æœ€æ–°: {title[:20]}...")
                    except Exception as e:
                        url_titles.append((url, f"å¤„ç†å‡ºé”™: {str(e)}"))
                        pbar.update(1)
    else:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(fetch_title, url, True): url for url in urls}
            for future in as_completed(futures):
                url = futures[future]
                try:
                    url, title = future.result()
                    url_titles.append((url, title))
                except Exception as e:
                    print(f"âŒ å¤„ç† {url} æ—¶å‡ºé”™: {e}")
                    url_titles.append((url, f"å¤„ç†å‡ºé”™: {str(e)}"))

    return url_titles


def save_sitemap_data(sitemap_urls, sitemap_titles=None):
    """ä¿å­˜æ¯ä¸ªsitemapçš„æ•°æ®åˆ°å•ç‹¬æ–‡ä»¶"""
    _, output_dir = ensure_dirs()

    for path, urls in sitemap_urls.items():
        # ç”ŸæˆåŸºç¡€æ–‡ä»¶å
        if path.startswith(('http://', 'https://')):
            base_name = sanitize_filename(path.split('//')[1].replace('/', '_'))
        else:
            base_name = sanitize_filename(os.path.basename(path).replace('.xml', ''))

        # ä¿å­˜URLåˆ—è¡¨
        urls_filename = os.path.join(output_dir, f"{base_name}â€”urls.txt")
        try:
            with open(urls_filename, 'w', encoding='utf-8') as f:
                f.writelines(url + '\n' for url in urls)
            print(f"âœ… {path} çš„URLåˆ—è¡¨å·²ä¿å­˜åˆ° {urls_filename}")
        except Exception as e:
            print(f"âŒ {path} çš„URLåˆ—è¡¨ä¿å­˜å¤±è´¥: {e}")

        # ä¿å­˜å¸¦æ ‡é¢˜çš„URLåˆ—è¡¨ï¼ˆå¦‚æœæä¾›ï¼‰
        if sitemap_titles and path in sitemap_titles:
            titles_filename = os.path.join(output_dir, f"{base_name}â€”UrlsWithTitles.txt")
            try:
                with open(titles_filename, 'w', encoding='utf-8') as f:
                    for url, title in sitemap_titles[path]:
                        f.write(f"{url}\t{title}\n")
                print(f"âœ… {path} çš„å¸¦æ ‡é¢˜URLåˆ—è¡¨å·²ä¿å­˜åˆ° {titles_filename}")
            except Exception as e:
                print(f"âŒ {path} çš„å¸¦æ ‡é¢˜URLåˆ—è¡¨ä¿å­˜å¤±è´¥: {e}")


def get_progress_preference():
    """è·å–ç”¨æˆ·å¯¹è¿›åº¦æ˜¾ç¤ºæ–¹å¼çš„åå¥½"""
    print("\n=== è¿›åº¦æ˜¾ç¤ºé€‰é¡¹ ===")
    print("1. è¿›åº¦æ¡æ¨¡å¼ (é€‚åˆå¤§é‡URL)")
    print("2. é€ä¸ªè¾“å‡ºæ¨¡å¼ (é€‚åˆå°‘é‡URL)")

    while True:
        choice = input("è¯·é€‰æ‹©è¿›åº¦æ˜¾ç¤ºæ–¹å¼ (1/2): ").strip()
        if choice == '1':
            return 'bar'
        elif choice == '2':
            return 'verbose'
        else:
            print("âŒ æ— æ•ˆè¾“å…¥ï¼Œè¯·é€‰æ‹©1æˆ–2")


if __name__ == "__main__":
    try:
        configure_console()
        paths = get_sitemap_paths()
        sitemap_urls = parse_multiple_sitemaps(paths)

        if not sitemap_urls:
            print("âŒ æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆURL")
            sys.exit(1)

        total_urls = sum(len(urls) for urls in sitemap_urls.values())
        print(f"\nâœ… æˆåŠŸæå– {total_urls} ä¸ªURLï¼ˆæ¥è‡ª {len(sitemap_urls)} ä¸ªsitemapï¼‰")

        # ä¿å­˜åŸå§‹URLåˆ—è¡¨ï¼ˆæ¯ä¸ªsitemapå•ç‹¬æ–‡ä»¶ï¼‰
        save_sitemap_data(sitemap_urls)

        # è¯¢é—®ç”¨æˆ·æ˜¯å¦éœ€è¦è·å–æ ‡é¢˜
        get_titles = input("\næ˜¯å¦è¦è·å–ç½‘é¡µæ ‡é¢˜ï¼Ÿ(y/n, é»˜è®¤y): ").strip().lower()
        if get_titles in ('y', 'yes', ''):
            progress_type = get_progress_preference()
            start_time = time.time()

            # ä¸ºæ¯ä¸ªsitemapè·å–æ ‡é¢˜
            sitemap_titles = {}
            for path, urls in sitemap_urls.items():
                print(f"\næ­£åœ¨å¤„ç† {path} çš„URL...")
                url_titles = fetch_titles_for_sitemap(urls, progress_type=progress_type)
                sitemap_titles[path] = url_titles

            elapsed_time = time.time() - start_time

            # ä¿å­˜å¸¦æ ‡é¢˜çš„URLåˆ—è¡¨ï¼ˆæ¯ä¸ªsitemapå•ç‹¬æ–‡ä»¶ï¼‰
            save_sitemap_data(sitemap_urls, sitemap_titles)

            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            print(f"\nâ±ï¸ å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
            success_count = sum(
                1 for titles in sitemap_titles.values()
                for _, title in titles
                if not title.startswith('è·å–æ ‡é¢˜å¤±è´¥')
            )
            print(f"ğŸ“Š ç»Ÿè®¡: {total_urls}ä¸ªURL | æˆåŠŸè·å–æ ‡é¢˜: {success_count}")

            # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
            for path in list(sitemap_urls.keys())[:3]:  # æœ€å¤šæ˜¾ç¤ºå‰3ä¸ªsitemapçš„æ ·ä¾‹
                print(f"\n=== {path} çš„ç»“æœæ ·ä¾‹ ===")
                for url, title in sitemap_titles[path][:5]:
                    print(f"{url} - {title}")
                if len(sitemap_titles[path]) > 5:
                    print(f"...ï¼ˆå…± {len(sitemap_titles[path])} æ¡ï¼Œå®Œæ•´ç»“æœè§æ–‡ä»¶ï¼‰")
        else:
            print("\nå·²è·³è¿‡è·å–æ ‡é¢˜æ­¥éª¤")
            # æ˜¾ç¤ºéƒ¨åˆ†URLæ ·ä¾‹
            for path in list(sitemap_urls.keys())[:3]:
                print(f"\n=== {path} çš„URLæ ·ä¾‹ ===")
                print('\n'.join(sitemap_urls[path][:5]))
                if len(sitemap_urls[path]) > 5:
                    print(f"...ï¼ˆå…± {len(sitemap_urls[path])} æ¡ï¼Œå®Œæ•´ç»“æœè§æ–‡ä»¶ï¼‰")

    except KeyboardInterrupt:
        print("\næ“ä½œå·²å–æ¶ˆ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}")
        sys.exit(1)